GPU available: True, used: True
TPU available: None, using: 0 TPU cores
initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/3
initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/3
wandb: Currently logged in as: naddeok (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.10.22 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-03-11 10:01:01.616308: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-11 10:01:01.618912: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
wandb: Tracking run with wandb version 0.10.21
wandb: Syncing run no_U
wandb: ‚≠êÔ∏è View project at https://wandb.ai/naddeok/LeNet%20CIFAR10%20Lightning
wandb: üöÄ View run at https://wandb.ai/naddeok/LeNet%20CIFAR10%20Lightning/runs/9mvdxxv0
wandb: Run data is saved locally in /home/naddeok5/FIM/wandb/run-20210311_100059-9mvdxxv0
wandb: Run `wandb offline` to turn off syncing.
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/3
Process wandb_internal:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/naddeok5/.local/lib/python3.8/site-packages/wandb/sdk/internal/internal.py", line 153, in wandb_internal
    thread.join()
  File "/usr/lib/python3.8/threading.py", line 1011, in join
    self._wait_for_tstate_lock()
  File "/usr/lib/python3.8/threading.py", line 1027, in _wait_for_tstate_lock
    elif lock.acquire(block, timeout):
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/naddeok5/FIM/lit_lenet_trainer.py", line 60, in <module>
    trainer.fit(net)
  File "/home/naddeok5/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 510, in fit
    self.pre_dispatch()
  File "/home/naddeok5/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 539, in pre_dispatch
    self.accelerator.pre_dispatch()
  File "/home/naddeok5/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py", line 84, in pre_dispatch
    self.training_type_plugin.pre_dispatch()
  File "/home/naddeok5/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py", line 233, in pre_dispatch
    self.init_ddp_connection(self.global_rank, self.world_size)
  File "/home/naddeok5/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py", line 216, in init_ddp_connection
    torch_distrib.init_process_group(self.torch_distributed_backend, rank=global_rank, world_size=world_size)
  File "/usr/lib/python3/dist-packages/torch/distributed/distributed_c10d.py", line 455, in init_process_group
    barrier()
  File "/usr/lib/python3/dist-packages/torch/distributed/distributed_c10d.py", line 1960, in barrier
    work = _default_pg.barrier()
KeyboardInterrupt
Traceback (most recent call last):
  File "lit_lenet_trainer.py", line 60, in <module>
    trainer.fit(net)
  File "/home/naddeok5/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 510, in fit
    self.pre_dispatch()
  File "/home/naddeok5/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 539, in pre_dispatch
    self.accelerator.pre_dispatch()
  File "/home/naddeok5/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py", line 84, in pre_dispatch
    self.training_type_plugin.pre_dispatch()
  File "/home/naddeok5/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py", line 233, in pre_dispatch
    self.init_ddp_connection(self.global_rank, self.world_size)
  File "/home/naddeok5/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py", line 216, in init_ddp_connection
    torch_distrib.init_process_group(self.torch_distributed_backend, rank=global_rank, world_size=world_size)
  File "/usr/lib/python3/dist-packages/torch/distributed/distributed_c10d.py", line 455, in init_process_group
    barrier()
  File "/usr/lib/python3/dist-packages/torch/distributed/distributed_c10d.py", line 1960, in barrier
    work = _default_pg.barrier()
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/naddeok5/FIM/lit_lenet_trainer.py", line 60, in <module>
    trainer.fit(net)
  File "/home/naddeok5/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 510, in fit
    self.pre_dispatch()
  File "/home/naddeok5/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 539, in pre_dispatch
    self.accelerator.pre_dispatch()
  File "/home/naddeok5/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py", line 84, in pre_dispatch
    self.training_type_plugin.pre_dispatch()
  File "/home/naddeok5/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py", line 233, in pre_dispatch
    self.init_ddp_connection(self.global_rank, self.world_size)
  File "/home/naddeok5/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py", line 216, in init_ddp_connection
    torch_distrib.init_process_group(self.torch_distributed_backend, rank=global_rank, world_size=world_size)
  File "/usr/lib/python3/dist-packages/torch/distributed/distributed_c10d.py", line 455, in init_process_group
    barrier()
  File "/usr/lib/python3/dist-packages/torch/distributed/distributed_c10d.py", line 1960, in barrier
    work = _default_pg.barrier()
KeyboardInterrupt
wandb: Waiting for W&B process to finish, PID 4002262
wandb: Program failed with code 255.  Press ctrl-c to abort syncing.
wandb: ERROR Problem finishing run
/usr/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 6 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
