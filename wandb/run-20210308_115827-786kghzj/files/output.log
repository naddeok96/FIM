/home/naddeok5/.local/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Checkpoint directory /home/naddeok5/FIM/models/pretrained/ exists and is not empty.
  warnings.warn(*args, **kwargs)
GPU available: True, used: True
--- Logging error ---
Traceback (most recent call last):
  File "/usr/lib/python3.8/logging/__init__.py", line 1085, in emit
    self.flush()
  File "/usr/lib/python3.8/logging/__init__.py", line 1065, in flush
    self.stream.flush()
ValueError: I/O operation on closed file.
Call stack:
  File "/usr/lib/python3.8/threading.py", line 890, in _bootstrap
    self._bootstrap_inner()
  File "/usr/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/naddeok5/.local/lib/python3.8/site-packages/wandb/agents/pyagent.py", line 303, in _run_job
    self._function()
  File "mnist_lighting_trainer.py", line 44, in sweep_iteration
    trainer = pl.Trainer(logger=wandb_logger,
  File "/home/naddeok5/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py", line 42, in overwrite_by_env_vars
    return fn(self, **kwargs)
  File "/home/naddeok5/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 318, in __init__
    self.accelerator_connector = AcceleratorConnector(
  File "/home/naddeok5/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 119, in __init__
    self.set_distributed_mode()
  File "/home/naddeok5/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 542, in set_distributed_mode
    rank_zero_info(f'GPU available: {torch.cuda.is_available()}, used: {self._device_type == DeviceType.GPU}')
  File "/home/naddeok5/.local/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py", line 40, in wrapped_fn
    return fn(*args, **kwargs)
  File "/home/naddeok5/.local/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py", line 54, in _info
    log.info(*args, **kwargs)
Message: 'GPU available: True, used: True'
Arguments: ()
TPU available: None, using: 0 TPU cores
--- Logging error ---
Traceback (most recent call last):
  File "/usr/lib/python3.8/logging/__init__.py", line 1085, in emit
    self.flush()
  File "/usr/lib/python3.8/logging/__init__.py", line 1065, in flush
    self.stream.flush()
ValueError: I/O operation on closed file.
Call stack:
  File "/usr/lib/python3.8/threading.py", line 890, in _bootstrap
    self._bootstrap_inner()
  File "/usr/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/naddeok5/.local/lib/python3.8/site-packages/wandb/agents/pyagent.py", line 303, in _run_job
    self._function()
  File "mnist_lighting_trainer.py", line 44, in sweep_iteration
    trainer = pl.Trainer(logger=wandb_logger,
  File "/home/naddeok5/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py", line 42, in overwrite_by_env_vars
    return fn(self, **kwargs)
  File "/home/naddeok5/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 318, in __init__
    self.accelerator_connector = AcceleratorConnector(
  File "/home/naddeok5/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 119, in __init__
    self.set_distributed_mode()
  File "/home/naddeok5/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 544, in set_distributed_mode
    rank_zero_info(f'TPU available: {_TPU_AVAILABLE}, using: {num_cores} TPU cores')
  File "/home/naddeok5/.local/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py", line 40, in wrapped_fn
    return fn(*args, **kwargs)
  File "/home/naddeok5/.local/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py", line 54, in _info
    log.info(*args, **kwargs)
Message: 'TPU available: None, using: 0 TPU cores'
Arguments: ()
