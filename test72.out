Unable to init server: Could not connect: Connection refused
Unable to init server: Could not connect: Connection refused

(wandb_model_trainer.py:3483682): Gdk-CRITICAL **: 21:04:33.360: gdk_cursor_new_for_display: assertion 'GDK_IS_DISPLAY (display)' failed
wandb: Agent Starting Run: 6oum2kky with config:
wandb: 	batch_size: 10
wandb: 	criterion: cross_entropy
wandb: 	epochs: 200
wandb: 	learning_rate: 0.015595416913914245
wandb: 	model_name: cifar10_mobilenetv2_x1_4
wandb: 	momentum: 0.9
wandb: 	optimizer: nesterov
wandb: 	pretrained: False
wandb: 	scheduler: Cosine Annealing
wandb: 	transformation: models/pretrained/U_w_means_0-005174736492335796_n0-0014449692098423839_n0-0010137659264728427_and_stds_1-130435824394226_1-128873586654663_1-1922636032104492_.pt
wandb: 	use_SAM: True
wandb: 	weight_decay: 1e-05
wandb: Currently logged in as: naddeok (use `wandb login --relogin` to force relogin)
Files already downloaded and verified
Files already downloaded and verified
CIFAR10 is Loaded
Create sweep with ID: 5xv82utr
Sweep URL: https://wandb.ai/naddeok/CIFAR10/sweeps/5xv82utr
wandb: wandb version 0.10.31 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-05-31 21:04:37.941191: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-05-31 21:04:37.943407: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
wandb: Tracking run with wandb version 0.10.30
wandb: Syncing run gallant-sweep-1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/naddeok/CIFAR10
wandb: üßπ View sweep at https://wandb.ai/naddeok/CIFAR10/sweeps/5xv82utr
wandb: üöÄ View run at https://wandb.ai/naddeok/CIFAR10/runs/6oum2kky
wandb: Run data is saved locally in /home/naddeok5/FIM/wandb/run-20210531_210436-6oum2kky
wandb: Run `wandb offline` to turn off syncing.

signal only works in main thread
Using cache found in /home/naddeok5/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master
wandb: Waiting for W&B process to finish, PID 3483847
wandb: Program failed with code 1.  Press ctrl-c to abort syncing.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/FIM/wandb/run-20210531_210436-6oum2kky/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/FIM/wandb/run-20210531_210436-6oum2kky/logs/debug-internal.log
wandb: Run summary:
wandb:   Data Augmentation False
wandb:            _runtime 3
wandb:          _timestamp 1622509479
wandb:               _step 0
wandb: Run history:
wandb:   Data Augmentation ‚ñÅ
wandb:            _runtime ‚ñÅ
wandb:          _timestamp ‚ñÅ
wandb:               _step ‚ñÅ
wandb: 
wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced gallant-sweep-1: https://wandb.ai/naddeok/CIFAR10/runs/6oum2kky
Run 6oum2kky errored: RuntimeError('CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 23.65 GiB total capacity; 1.75 GiB already allocated; 185.44 MiB free; 1.86 GiB reserved in total by PyTorch)')
wandb: ERROR Run 6oum2kky errored: RuntimeError('CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 23.65 GiB total capacity; 1.75 GiB already allocated; 185.44 MiB free; 1.86 GiB reserved in total by PyTorch)')
wandb: Agent Starting Run: lxm6dzxh with config:
wandb: 	batch_size: 14
wandb: 	criterion: cross_entropy
wandb: 	epochs: 200
wandb: 	learning_rate: 0.01916924978925024
wandb: 	model_name: cifar10_mobilenetv2_x1_4
wandb: 	momentum: 0.9
wandb: 	optimizer: nesterov
wandb: 	pretrained: False
wandb: 	scheduler: Cosine Annealing
wandb: 	transformation: models/pretrained/U_w_means_0-005174736492335796_n0-0014449692098423839_n0-0010137659264728427_and_stds_1-130435824394226_1-128873586654663_1-1922636032104492_.pt
wandb: 	use_SAM: True
wandb: 	weight_decay: 1e-05

wandb: wandb version 0.10.31 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-05-31 21:12:43.244659: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-05-31 21:12:43.247360: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
wandb: Tracking run with wandb version 0.10.30
wandb: Syncing run electric-sweep-2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/naddeok/CIFAR10
wandb: üßπ View sweep at https://wandb.ai/naddeok/CIFAR10/sweeps/5xv82utr
wandb: üöÄ View run at https://wandb.ai/naddeok/CIFAR10/runs/lxm6dzxh
wandb: Run data is saved locally in /home/naddeok5/FIM/wandb/run-20210531_211241-lxm6dzxh
wandb: Run `wandb offline` to turn off syncing.

signal only works in main thread
Using cache found in /home/naddeok5/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master
wandb: Waiting for W&B process to finish, PID 3488368
wandb: Program failed with code 1.  Press ctrl-c to abort syncing.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/FIM/wandb/run-20210531_211241-lxm6dzxh/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/FIM/wandb/run-20210531_211241-lxm6dzxh/logs/debug-internal.log
wandb: Run summary:
wandb:   Data Augmentation False
wandb:            _runtime 3
wandb:          _timestamp 1622509964
wandb:               _step 0
wandb: Run history:
wandb:   Data Augmentation ‚ñÅ
wandb:            _runtime ‚ñÅ
wandb:          _timestamp ‚ñÅ
wandb:               _step ‚ñÅ
wandb: 
wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced electric-sweep-2: https://wandb.ai/naddeok/CIFAR10/runs/lxm6dzxh
Run lxm6dzxh errored: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.65 GiB total capacity; 2.01 GiB already allocated; 19.44 MiB free; 2.02 GiB reserved in total by PyTorch)')
wandb: ERROR Run lxm6dzxh errored: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.65 GiB total capacity; 2.01 GiB already allocated; 19.44 MiB free; 2.02 GiB reserved in total by PyTorch)')
wandb: Agent Starting Run: 64sjpg4q with config:
wandb: 	batch_size: 26
wandb: 	criterion: cross_entropy
wandb: 	epochs: 200
wandb: 	learning_rate: 0.006665133950209708
wandb: 	model_name: cifar10_mobilenetv2_x1_4
wandb: 	momentum: 0.9
wandb: 	optimizer: nesterov
wandb: 	pretrained: False
wandb: 	scheduler: Cosine Annealing
wandb: 	transformation: models/pretrained/U_w_means_0-005174736492335796_n0-0014449692098423839_n0-0010137659264728427_and_stds_1-130435824394226_1-128873586654663_1-1922636032104492_.pt
wandb: 	use_SAM: True
wandb: 	weight_decay: 1e-05

wandb: wandb version 0.10.31 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-05-31 21:12:51.499096: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-05-31 21:12:51.501363: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
wandb: Tracking run with wandb version 0.10.30
wandb: Syncing run cosmic-sweep-3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/naddeok/CIFAR10
wandb: üßπ View sweep at https://wandb.ai/naddeok/CIFAR10/sweeps/5xv82utr
wandb: üöÄ View run at https://wandb.ai/naddeok/CIFAR10/runs/64sjpg4q
wandb: Run data is saved locally in /home/naddeok5/FIM/wandb/run-20210531_211249-64sjpg4q
wandb: Run `wandb offline` to turn off syncing.
Using cache found in /home/naddeok5/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master
wandb: Waiting for W&B process to finish, PID 3488583
wandb: Program failed with code 1.  Press ctrl-c to abort syncing.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/FIM/wandb/run-20210531_211249-64sjpg4q/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/FIM/wandb/run-20210531_211249-64sjpg4q/logs/debug-internal.log
wandb: Run summary:
wandb:   Data Augmentation False
wandb:            _runtime 3
wandb:          _timestamp 1622509972
wandb:               _step 0
wandb: Run history:
wandb:   Data Augmentation ‚ñÅ
wandb:            _runtime ‚ñÅ
wandb:          _timestamp ‚ñÅ
wandb:               _step ‚ñÅ
wandb: 
wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced cosmic-sweep-3: https://wandb.ai/naddeok/CIFAR10/runs/64sjpg4q
Run 64sjpg4q errored: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.65 GiB total capacity; 2.02 GiB already allocated; 17.44 MiB free; 2.02 GiB reserved in total by PyTorch)')
wandb: ERROR Run 64sjpg4q errored: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.65 GiB total capacity; 2.02 GiB already allocated; 17.44 MiB free; 2.02 GiB reserved in total by PyTorch)')
wandb: Agent Starting Run: 3ywj7ocr with config:
wandb: 	batch_size: 30
wandb: 	criterion: cross_entropy
wandb: 	epochs: 200
wandb: 	learning_rate: 0.004445035000248893
wandb: 	model_name: cifar10_mobilenetv2_x1_4
wandb: 	momentum: 0.9
wandb: 	optimizer: nesterov
wandb: 	pretrained: False
wandb: 	scheduler: Cosine Annealing
wandb: 	transformation: models/pretrained/U_w_means_0-005174736492335796_n0-0014449692098423839_n0-0010137659264728427_and_stds_1-130435824394226_1-128873586654663_1-1922636032104492_.pt
wandb: 	use_SAM: True
wandb: 	weight_decay: 1e-05

signal only works in main thread

wandb: wandb version 0.10.31 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-05-31 21:12:58.755439: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-05-31 21:12:58.758185: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
wandb: Tracking run with wandb version 0.10.30
wandb: Syncing run lilac-sweep-4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/naddeok/CIFAR10
wandb: üßπ View sweep at https://wandb.ai/naddeok/CIFAR10/sweeps/5xv82utr
wandb: üöÄ View run at https://wandb.ai/naddeok/CIFAR10/runs/3ywj7ocr
wandb: Run data is saved locally in /home/naddeok5/FIM/wandb/run-20210531_211257-3ywj7ocr
wandb: Run `wandb offline` to turn off syncing.
Using cache found in /home/naddeok5/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master
wandb: Waiting for W&B process to finish, PID 3488761
wandb: Program failed with code 1.  Press ctrl-c to abort syncing.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/FIM/wandb/run-20210531_211257-3ywj7ocr/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/FIM/wandb/run-20210531_211257-3ywj7ocr/logs/debug-internal.log
wandb: Run summary:
wandb:   Data Augmentation False
wandb:            _runtime 3
wandb:          _timestamp 1622509980
wandb:               _step 0
wandb: Run history:
wandb:   Data Augmentation ‚ñÅ
wandb:            _runtime ‚ñÅ
wandb:          _timestamp ‚ñÅ
wandb:               _step ‚ñÅ
wandb: 
wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced lilac-sweep-4: https://wandb.ai/naddeok/CIFAR10/runs/3ywj7ocr
Run 3ywj7ocr errored: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.65 GiB total capacity; 2.02 GiB already allocated; 11.44 MiB free; 2.03 GiB reserved in total by PyTorch)')
wandb: ERROR Run 3ywj7ocr errored: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.65 GiB total capacity; 2.02 GiB already allocated; 11.44 MiB free; 2.03 GiB reserved in total by PyTorch)')
wandb: Agent Starting Run: rrt3554s with config:
wandb: 	batch_size: 27
wandb: 	criterion: cross_entropy
wandb: 	epochs: 200
wandb: 	learning_rate: 0.08593907992992735
wandb: 	model_name: cifar10_mobilenetv2_x1_4
wandb: 	momentum: 0.9
wandb: 	optimizer: nesterov
wandb: 	pretrained: False
wandb: 	scheduler: Cosine Annealing
wandb: 	transformation: models/pretrained/U_w_means_0-005174736492335796_n0-0014449692098423839_n0-0010137659264728427_and_stds_1-130435824394226_1-128873586654663_1-1922636032104492_.pt
wandb: 	use_SAM: True
wandb: 	weight_decay: 1e-05

signal only works in main thread

wandb: wandb version 0.10.31 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-05-31 21:13:06.921773: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-05-31 21:13:06.924066: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
wandb: Tracking run with wandb version 0.10.30
wandb: Syncing run upbeat-sweep-5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/naddeok/CIFAR10
wandb: üßπ View sweep at https://wandb.ai/naddeok/CIFAR10/sweeps/5xv82utr
wandb: üöÄ View run at https://wandb.ai/naddeok/CIFAR10/runs/rrt3554s
wandb: Run data is saved locally in /home/naddeok5/FIM/wandb/run-20210531_211305-rrt3554s
wandb: Run `wandb offline` to turn off syncing.
Using cache found in /home/naddeok5/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master
wandb: Waiting for W&B process to finish, PID 3488942
wandb: Program failed with code 1.  Press ctrl-c to abort syncing.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/FIM/wandb/run-20210531_211305-rrt3554s/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/FIM/wandb/run-20210531_211305-rrt3554s/logs/debug-internal.log
wandb: Run summary:
wandb:   Data Augmentation False
wandb:            _runtime 3
wandb:          _timestamp 1622509988
wandb:               _step 0
wandb: Run history:
wandb:   Data Augmentation ‚ñÅ
wandb:            _runtime ‚ñÅ
wandb:          _timestamp ‚ñÅ
wandb:               _step ‚ñÅ
wandb: 
wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced upbeat-sweep-5: https://wandb.ai/naddeok/CIFAR10/runs/rrt3554s
Run rrt3554s errored: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.65 GiB total capacity; 2.03 GiB already allocated; 5.44 MiB free; 2.03 GiB reserved in total by PyTorch)')
wandb: ERROR Run rrt3554s errored: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.65 GiB total capacity; 2.03 GiB already allocated; 5.44 MiB free; 2.03 GiB reserved in total by PyTorch)')
wandb: Agent Starting Run: wmz5k0ye with config:
wandb: 	batch_size: 13
wandb: 	criterion: cross_entropy
wandb: 	epochs: 200
wandb: 	learning_rate: 0.09656120926097826
wandb: 	model_name: cifar10_mobilenetv2_x1_4
wandb: 	momentum: 0.9
wandb: 	optimizer: nesterov
wandb: 	pretrained: False
wandb: 	scheduler: Cosine Annealing
wandb: 	transformation: models/pretrained/U_w_means_0-005174736492335796_n0-0014449692098423839_n0-0010137659264728427_and_stds_1-130435824394226_1-128873586654663_1-1922636032104492_.pt
wandb: 	use_SAM: True
wandb: 	weight_decay: 1e-05

signal only works in main thread

wandb: wandb version 0.10.31 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-05-31 21:13:14.298431: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-05-31 21:13:14.301434: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
wandb: Tracking run with wandb version 0.10.30
wandb: Syncing run revived-sweep-6
wandb: ‚≠êÔ∏è View project at https://wandb.ai/naddeok/CIFAR10
wandb: üßπ View sweep at https://wandb.ai/naddeok/CIFAR10/sweeps/5xv82utr
wandb: üöÄ View run at https://wandb.ai/naddeok/CIFAR10/runs/wmz5k0ye
wandb: Run data is saved locally in /home/naddeok5/FIM/wandb/run-20210531_211312-wmz5k0ye
wandb: Run `wandb offline` to turn off syncing.
Using cache found in /home/naddeok5/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master
wandb: Waiting for W&B process to finish, PID 3489120
wandb: Program failed with code 1.  Press ctrl-c to abort syncing.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/FIM/wandb/run-20210531_211312-wmz5k0ye/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/FIM/wandb/run-20210531_211312-wmz5k0ye/logs/debug-internal.log
wandb: Run summary:
wandb:   Data Augmentation False
wandb:            _runtime 3
wandb:          _timestamp 1622509995
wandb:               _step 0
wandb: Run history:
wandb:   Data Augmentation ‚ñÅ
wandb:            _runtime ‚ñÅ
wandb:          _timestamp ‚ñÅ
wandb:               _step ‚ñÅ
wandb: 
wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced revived-sweep-6: https://wandb.ai/naddeok/CIFAR10/runs/wmz5k0ye
Run wmz5k0ye errored: RuntimeError('CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 23.65 GiB total capacity; 2.03 GiB already allocated; 3.44 MiB free; 2.03 GiB reserved in total by PyTorch)')
wandb: ERROR Run wmz5k0ye errored: RuntimeError('CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 23.65 GiB total capacity; 2.03 GiB already allocated; 3.44 MiB free; 2.03 GiB reserved in total by PyTorch)')
Detected 5 failed runs in a row at start, killing sweep.
wandb: ERROR Detected 5 failed runs in a row at start, killing sweep.
wandb: To change this value set WANDB_AGENT_MAX_INITIAL_FAILURES=val

signal only works in main thread

